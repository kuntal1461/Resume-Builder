# Job Recommendation System ğŸš€

This repository powers an **AI-driven job recommendation platform**.  
The system lets a user **log in**, **upload a resume**, then an **LLM generates 7â€“8 tailored questions**. The answers, combined with resume data, decide the **userâ€™s score**:

- If **score < 60** â†’ show popup warning, user can retry.
- If **score â‰¥ 60** â†’ system scrapes job data from **LinkedIn** and uses **LangChain-powered LLMs** to **recommend jobs**.

---

## ğŸ“‚ Repository Structure


.
â”œâ”€â”€ job-recommendation-framework/ # Framework: AI & LangChain
â”‚ â”œâ”€â”€ framework/
â”‚ â”‚ â”œâ”€â”€ llm/ # LLM prompts, chains, providers
â”‚ â”‚ â”œâ”€â”€ parsers/ # LLM-assisted parsing
â”‚ â”‚ â”œâ”€â”€ qna/ # Question generation & scoring
â”‚ â”‚ â”œâ”€â”€ recsys/ # Job reranking logic
â”‚ â”‚ â””â”€â”€ interfaces/ # Clean contracts for the system
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ job-recommendation-system/ # Main system (core, API, frontend)
â”‚ â”œâ”€â”€ core/ # Resume parsing, scoring, jobs logic
â”‚ â”œâ”€â”€ web/ # FastAPI/Flask APIs (/api/v1/resumesystem/*)
â”‚ â”œâ”€â”€ frontend/ # React-based UI (login, Q&A, jobs)
â”‚ â”œâ”€â”€ integrators/ # LinkedIn scraper / API adapter
â”‚ â”œâ”€â”€ data/ # DB migrations, seeds
â”‚ â”œâ”€â”€ configs/ # Configs & env files
â”‚ â””â”€â”€ README.md
â”‚
â””â”€â”€ README.md # Root project overview (this file)



---

## ğŸ§­ High-Level Flow

flowchart TD
  subgraph FE["Frontend (React)"]
    FE1["Login"]
    FE2["Upload Resume"]
    FE3["Interactive Q&A (7-8 questions)"]
    FE4["Show Score & Decision"]
    FE5["Popup: Score < 60"]
    FE6["Jobs UI (Recommendations >= 60)"]
  end

  subgraph API["Web API (FastAPI/Flask)"]
    A1["Auth & Session"]
    A2["POST /resume"]
    A3["POST /qa/answer"]
    A4["GET /score"]
    A5["GET /jobs/recommend"]
  end

  subgraph CORE["Core Business Logic"]
    C1["Resume Parser (PDF/DOCX â†’ JSON)"]
    C2["Profile Normalizer"]
    C3["Question Generator Orchestrator"]
    C4["Q&A Evaluator"]
    C5["Scoring Engine"]
    C6["Job Recommender Orchestrator"]
  end

  subgraph FW["Framework (LangChain + LLMs)"]
    F1["LLM Parsing Assist"]
    F2["LLM Question Generator"]
    F3["LLM Answer Scoring"]
    F4["LLM Job Reranker"]
  end

  subgraph SRC["Job Source"]
    S1["LinkedIn Scraper / API"]
  end

  FE1 --> A1 --> FE2
  FE2 -->|Upload file| A2 --> C1
  C1 --> C2 --> A2
  C1 -. assist .-> F1
  A2 --> C3 -. uses .-> F2
  C3 --> FE3
  FE3 -->|Answers| A3 --> C4 -. uses .-> F3
  C4 --> C5 --> FE4
  FE4 -->|Score < 60| FE5
  FE4 -->|Score >= 60| A5 --> C6
  C6 -->|Fetch jobs| S1
  C6 -. rerank .-> F4
  C6 --> FE6

<img width="3840" height="2656" alt="image" src="https://github.com/user-attachments/assets/d3c373ec-e120-40d3-9fb5-0204b513d93e" />


ğŸ”Œ Module Responsibilities
job-recommendation-framework
LLM-based resume parsing, question generation, answer scoring, job reranking.
Independent package â€” plug & play with system.
job-recommendation-system
Core resume parser (non-LLM fallback).
Scoring logic (resume + answers).
Job fetching (LinkedIn scraper/integrator).
API layer (/api/v1/resumesystem/*).
React frontend (Login, Resume upload, Q&A, Jobs view).



âš™ï¸ Tech Stack
Backend: Python 3.9.6, FastAPI (or Flask)
Frontend: Nxt.Js + Tailwind
AI: LangChain + LLMs
DBs: PostgreSQL/MySQL, FAISS/PGVector, Redis/Elastic
Scraping: LinkedIn (with compliance checks)
Infra: Docker optional, .env configs for secrets



ğŸ§ª Scoring Logic
Final Score = 0.6 Ã— Resume Score + 0.4 Ã— Q&A Score
Threshold:
< 60 â†’ popup: â€œYour score is below 60â€
â‰¥ 60 â†’ LinkedIn scraping + job recommendations



ğŸ§µ Sequence Diagram (Q&A Round)


sequenceDiagram
  autonumber
  participant FE as Frontend
  participant API as Web API
  participant CORE as Core
  participant FW as Framework
  participant DB as DB

  FE->>API: POST /resume (file)
  API->>CORE: parse_and_normalize(uri)
  CORE->>FW: LLM.extract_entities(text)
  FW-->>CORE: entities
  CORE->>DB: save profile
  API-->>FE: profileId

  FE->>API: GET /qa/start
  API->>CORE: generate_questions(profile)
  CORE->>FW: LLM.qgen(profile)
  FW-->>CORE: 7-8 questions
  CORE-->>API: questions
  API-->>FE: show questions

  loop For each answer
    FE->>API: POST /qa/answer
    API->>CORE: score_answer
    CORE->>FW: LLM.score(q,a,profile)
    FW-->>CORE: score
    CORE->>DB: save partial score
  end

  FE->>API: GET /score
  API->>CORE: compute_score
  CORE-->>API: score
  API-->>FE: final score + decision
